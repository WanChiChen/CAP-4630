{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXMlrevyk91onloIvG1229"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xu6Kn7Ag9_p",
        "colab_type": "text"
      },
      "source": [
        "# Homework 5\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TZMHsZchVEH",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "Summarize and describe the different concepts/methods/algorithms that you have learned in this course.\n",
        "\n",
        "Use a Colab notebook. Make sure that you organize the material logically by using sections/subsections. Also, use code cell to include code snippets.\n",
        "\n",
        "I suggest that you group everything into five categories:\n",
        "\n",
        " - General concepts (for instance, what is artificial intelligence, machine learning, deep learning)\n",
        "\n",
        " - Basic concepts (for instance, here you can talk about linear regression, logistic regression, gradients, gradient descent)\n",
        "\n",
        " - Building a model (for instance, here you can talk about the structure of a convent, what it components are etc.)\n",
        "\n",
        " - Comping a model (for instance, you can talk here about optimizers, learning rate etc.)\n",
        "\n",
        "- Training a model (for instance, you can talk about overfitting/underfitting)\n",
        "\n",
        "- Finetuning a pretrained model (describe how you proceed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggyYUPM4hX1R",
        "colab_type": "text"
      },
      "source": [
        "## General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH2ImEVhhsZ2",
        "colab_type": "text"
      },
      "source": [
        "### Artificial Intelligence\n",
        "\n",
        "- Umbrella term for any computer program that does something smart\n",
        "- \"The science and engineering of making intelligent machines\"\n",
        "- Deals with the simulation of intelligent behavior in computers\n",
        "- Uses input and rules to produce an output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiDqNaXjh8yo",
        "colab_type": "text"
      },
      "source": [
        "#### Machine Learning\n",
        "\n",
        "- Field of study that gives computers the ability to learn without being explicity programmed\n",
        "\n",
        "- Adjust themselves in response to data they are exposed to\n",
        "\n",
        "- Does not require human intervention to make certain changes\n",
        "- Uses inputs and expected outputs to produce its own rules\n",
        "\n",
        "      A computer program is said to learn from experience E with respect to \n",
        "\n",
        "      some class of tasks T and performance measure P if its performance at \n",
        "\n",
        "      tasks in T, as measured by P, improves with experience E."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4rDZpR1iAZ0",
        "colab_type": "text"
      },
      "source": [
        "##### Deep Learning\n",
        "\n",
        " - Subset of machine learning, uses networks to extract higher level features from the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeTDkTRvjz4o",
        "colab_type": "text"
      },
      "source": [
        "## Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGsjGMpTj4qM",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69uNLExclYV6",
        "colab_type": "text"
      },
      "source": [
        "A linear approach modeling the relationship between a dependent variable and independent variable(s)\n",
        "\n",
        "A basic model can be defined with the following equation:\n",
        "\n",
        "$\\quad \\quad \\hat{y} = b + w_1x_1$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\hat{y}$ is the predicted label (a desired output)\n",
        "\n",
        "- $b$ is the bias (the y-intercept), also known as $w_0$\n",
        "\n",
        "- $w_1$ is the weight of feature 1. Same concept as the slope m of an equation in slope-intercept form\n",
        "\n",
        "- $x_1$ is a feature (the input)\n",
        "\n",
        "It can be extended to multiple inputs (features):\n",
        "\n",
        "$\\quad \\quad \\hat{y} = b + \\sum\\limits_{j=1}^n w_jx_j$\n",
        "\n",
        "Where n is the number of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QlxFXXuj9wt",
        "colab_type": "text"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfx1W4p-or4J",
        "colab_type": "text"
      },
      "source": [
        "- Loss is the penalty for a bad prediction. It quantifies how bad the model's prediction was on a single example.\n",
        "\n",
        "- A perfect prediction yields a loss of zero, else its greater than 0.\n",
        "\n",
        "- Training a model means examining the examples and adjusting the weights and bias so that the loss is minimized.\n",
        "\n",
        "- Goal of training is to find a set of weights and biases that have low loss across all examples. This process is Empirical Risk Minimization.\n",
        "\n",
        "Mean Square Error (MSE) - average squared loss per example over whole data set\n",
        "\n",
        "$\\quad \\quad MSE(w) = \\frac{1}{m} \\sum\\limits_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$\n",
        "\n",
        "where:\n",
        "  - $m$ is the number of examples\n",
        "  - $x^{(i)}$ and $y^{(i)} $ are the features and the label of the ith example\n",
        "  - $\\hat{y}^{(i)}$ is the prediction of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ftK26pkkIOJ",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VfIYHkzqrRO",
        "colab_type": "text"
      },
      "source": [
        "Gradient Descent is how the model updates the learnable parameters during training.\n",
        "\n",
        "- Calculates the gradient of the loss function at some starting point\n",
        "  - A gradient always points in the direction of the steepest increase in the loss function\n",
        "\n",
        "- Gradient descent takes a step in the direction of the negative gradient to reduce the loss\n",
        "\n",
        "- Then updates each weight such that:\n",
        "\n",
        "  $\\quad w = w - \\alpha \\nabla \\mathcal{L}$\n",
        "- where $\\alpha$ is the learning rate, $\\mathcal{L}$ is the loss  function\n",
        "\n",
        "Gradient Descent can be done in batches of examples. Larger batches may cause a single iteration to take a very long time to compute. Smaller batches tend to have noisier gradients.\n",
        "\n",
        "- Stochastic Gradient Descent uses batch size = 1 per iteration. The example is chosen at random for each batch.\n",
        "\n",
        "- Mini-batch stochastic gradient descent typically choses between 10 and 10,000 examples at random for each batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cMxsVM7j7AZ",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njHH1cRYwrjJ",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression is used for binary classification problems.\n",
        "\n",
        "Binary Classification means there are only two possible classes, either positive or negative.\n",
        "\n",
        "Only one output neuron, whose activation indicates the probability of class 1.\n",
        "\n",
        "Logistic Regression uses the sigmoid function on the output neuron to map the values between 0 and 1.\n",
        "\n",
        "$\\quad \\sigma (z) = \\frac{1}{1+e^{-z}}$\n",
        "\n",
        "Either squared error loss or binary cross entropy loss can be used. The latter option speeds up training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh99nuLJk5ON",
        "colab_type": "text"
      },
      "source": [
        "## Building a Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwPXTm3UynGx",
        "colab_type": "text"
      },
      "source": [
        "- A neural network has 3 main layers: the input layer, the hidden layer, and the output layer\n",
        "\n",
        "- Each layer has an output size, input shape, and activation function.\n",
        "\n",
        "- Keras calculates the input shape of subsequent layers so that only the shape of the input has to be specified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRiCbNxry47H",
        "colab_type": "text"
      },
      "source": [
        "### The Input Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvhzZkgdy8oX",
        "colab_type": "text"
      },
      "source": [
        "The input layer receives the input for the model. The following is an example for adding an input layer to a model in Keras.\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(1024, activation = 'relu', input_shape = (128*128,)))\n",
        "\n",
        "where 1024 is the output size, the activation function used is ReLu, and the input shape is a 128 by 128 image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bskZe3t50l3o",
        "colab_type": "text"
      },
      "source": [
        "### The Hidden Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "010ZVxMc0pN1",
        "colab_type": "text"
      },
      "source": [
        "The hidden layer is comprised of all of the layers between the input and output. That is, there can be multiple layers in the hidden layer.\n",
        "\n",
        "These additional layers increase model complexity and help the model better extract features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAhgZSVe1FwT",
        "colab_type": "text"
      },
      "source": [
        "### The Output Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU5B1XHe1H-k",
        "colab_type": "text"
      },
      "source": [
        "The output layer yields the prediction that the model makes. \n",
        "\n",
        "In classification problems, one-hot encoding is used to show which label the example belongs to.\n",
        "\n",
        "In binary classification problems, the sigmoid activation function may be used, as it maps the output between 0 and 1.\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "The output size is 1 because it only needs to show the probability of one label in binary classification.\n",
        "\n",
        "In classification problems with more labels, the output size will need to increase to accomodate each label. The following example can be used as the output layer for a model trying to recognize the numbers 0 through 9.\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(10, activation = 'softmax'))\n",
        "\n",
        "There are 10 different labels for the example to map to, so 10 different vectors are needed to represent the probability of each one. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWb37NCr2qAV",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e74c5f0O4KjK",
        "colab_type": "text"
      },
      "source": [
        "- A breakthrough in building models for image classification\n",
        "- Can be used to progressively extract higher and higher level representations of image content\n",
        "- A CNN takes an image's raw pixel data as input and learns how to extract these features, and then infer what object they constitute\n",
        "- The input has 3 dimensions, length, width, and color\n",
        "- The input is convolved with different image filters, one for each feature, and then moves to the next layer\n",
        "- Different layer operations include pooling, activation, and additional convolutions\n",
        "- The filters used for convolution are the learnable parameters for the model, and are updated after each iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qB-xGa2k74o",
        "colab_type": "text"
      },
      "source": [
        "## Compiling a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DyWW0tm4Rhs",
        "colab_type": "text"
      },
      "source": [
        "### Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Z1B_Hg5APN",
        "colab_type": "text"
      },
      "source": [
        "Optimizers update the model in response to the output of the loss function by modifying the weights of the model. \n",
        "\n",
        "Examples include SGD (Stochastic Gradient Descent) and RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3NJAFBy4VFr",
        "colab_type": "text"
      },
      "source": [
        "### Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5inDb46E5nHJ",
        "colab_type": "text"
      },
      "source": [
        "The learning rate $\\alpha$ is a hyperparameter that controls how quickly the model learns by determining the next point of gradient descent. \n",
        "\n",
        "The gradient descent algorithm multiplies the gradient by the learning rate and then updates the weight such that:\n",
        "\n",
        "$\\quad w = w - \\alpha \\nabla \\mathcal{L}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-03kN0hg4hbN",
        "colab_type": "text"
      },
      "source": [
        "### Keras Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FmqyBoQ7KKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFobKDEE7Oe3",
        "colab_type": "text"
      },
      "source": [
        "Note that the optimizer is RMSprop and the loss function used is Categorical Cross Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSh1WOldk_Ad",
        "colab_type": "text"
      },
      "source": [
        "## Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5X69Ugl8qTQ",
        "colab_type": "text"
      },
      "source": [
        "###Overfitting and Underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3stabr-i8qdI",
        "colab_type": "text"
      },
      "source": [
        "Overfitting occurs when a model has low loss on the training data, but it has a high loss on the test data. This is caused by making the model more complex than necessary. \n",
        "\n",
        "Underfitting occurs when the model is not complex enough for the problem at hand, and will thus have high loss for both the training data and test data. \n",
        "\n",
        "The fundamental tension of machine learning is between fitting our training data well, but also fitting it as simply as possible.\n",
        "\n",
        "But to test if your model overfits, it needs to be run on previously unseen data. But how do you get this data?\n",
        "  - One solution is to divide the existing data set into two:\n",
        "      - A training set\n",
        "      - A test set\n",
        "  - Given a large enough test set, a good performance on the test set is a useful indicator of good performance on new data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTvPbM5s7eo5",
        "colab_type": "text"
      },
      "source": [
        "### Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i99a-P2Y7p9D",
        "colab_type": "text"
      },
      "source": [
        "- The number of epochs indicates the number of passes through the entire training dataset the machine learning algorithm will complete. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoVFJxhr7mdD",
        "colab_type": "text"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE1NZ-Is8QXW",
        "colab_type": "text"
      },
      "source": [
        "In this example, the model is trained on 60000 samples and validated on 10000 samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_tkskl78CMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs, \n",
        "                      batch_size=128, \n",
        "                      validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzkCfNgL8Jfe",
        "colab_type": "text"
      },
      "source": [
        "**Example Output:**\n",
        "\n",
        "Epoch 1/10\n",
        "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2553 - accuracy: 0.9267 - val_loss: 0.1188 - val_accuracy: 0.9659\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "Epoch 10/10\n",
        "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.0690 - val_accuracy: 0.9803"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zlMtiI5lAnb",
        "colab_type": "text"
      },
      "source": [
        "## Finetuning a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEknbJaE-uoX",
        "colab_type": "text"
      },
      "source": [
        "Fine-tuning allows us to further tune a pre-trained model to our specific problem or data set. \n",
        "\n",
        "Existing models available for fine-tuning include VGG16, ResNet50, and Xception.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKe_2XqP_eUa",
        "colab_type": "text"
      },
      "source": [
        "### Finetuning By Adding Additional Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtgZtTxJ_xqZ",
        "colab_type": "text"
      },
      "source": [
        "As seen in the example below, layers can be added to an existing ResNet model.\n",
        "\n",
        "This allows us to potentially improve performance by adding layers that are more helpful/specific to our problem. \n",
        "\n",
        "An output layer of size 1 using the sigmoid function was added below because the model was being used for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DxBSryAACVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import ResNet50\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "conv_base = ResNet50(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLTB_nnV_klb",
        "colab_type": "text"
      },
      "source": [
        "### Finetuning By Tuning the Pre-Made Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqQQP6PYAyWz",
        "colab_type": "text"
      },
      "source": [
        "We can fine-tune the layers of the pre-made meodel by freezing/unfreezing  certain layers in the model. \n",
        "\n",
        "By unfreezing a layer in the pre-made model, our model can train those layers by modifying its weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHRNE0XpAylr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}